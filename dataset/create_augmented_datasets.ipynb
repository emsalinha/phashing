{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image, PngImagePlugin\n",
    "import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=25)\n",
    "import scipy.fftpack\n",
    "import pandas as pd\n",
    "import time\n",
    "import h5py\n",
    "from collections import deque\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interlacing, compression, aspect ratio, kleur shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aleju/imgaug\n",
    "\n",
    "Add(V, PCH)\tAdds value V to each image. If PCH is true, then the sampled values may be different per channel.\n",
    "\n",
    "AdditiveGaussianNoise(L, S, PCH)\tAdds white/gaussian noise pixelwise to an image. The noise comes from the normal distribution N(L,S). If PCH is true, then the sampled values may be different per channel (and pixel).\n",
    "\n",
    "JpegCompression(C)\tApplies JPEG compression of strength C (value range: 0 to 100) to an image. Higher values of C lead to more visual artifacts.\n",
    "\n",
    "AddToHueAndSaturation(V, PCH, F, C)\tAdds value V to each pixel in HSV space (i.e. modifying hue and saturation). Converts from colorspace F to HSV (default is F=RGB). Selects channels C before augmenting (default is C=[0,1]). If PCH is true, then the sampled values may be different per channel.\n",
    "\n",
    "GammaContrast(G, PCH)\tApplies gamma contrast adjustment following I_ij' = I_ij**G', where G' is a gamma value sampled from G and I_ij a pixel (converted to 0 to 1.0 space). If PCH is true, a different G' is sampled per image and channel.\n",
    "\n",
    "PerspectiveTransform(S, KS)\tApplies a random four-point perspective transform to the image (kinda like an advanced form of cropping). Each point has a random distance from the image corner, derived from a normal distribution with sigma S. If KS is set to True (default), each image will be resized back to its original size.\n",
    "\n",
    "Resize(S, I)\tResizes images to size S. Common use case would be to use S={\"height\":H, \"width\":W} to resize all images to shape HxW. H and W may be floats (e.g. resize to 50% of original size). Either H or W may be \"keep-aspect-ratio\" to define only one side's new size and resize the other side correspondingly. I is the interpolation to use (default: cubic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = iaa.Sequential([iaa.Add(100, True)]) #-100: darker +100: lighter\n",
    "gauss = iaa.Sequential([iaa.AdditiveGaussianNoise(0, 20, True)]) #sigma determines maount of saltnpepper noise\n",
    "compress = iaa.Sequential([iaa.JpegCompression(96)]) #higher than 90\n",
    "add_hsv = iaa.Sequential(iaa.AddToHueAndSaturation(-15, True)) #-15 vs 15\n",
    "contrast = iaa.Sequential(iaa.GammaContrast(0.5, False)) #0.5 vs 2\n",
    "resize = iaa.Sequential(iaa.Resize({\"height\":0.8, \"width\":0.5})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(frame, hash_size, high_freq_factor, input_type = 'path'):\n",
    "    if input_type == 'path':\n",
    "        im = Image.open(frame_path).convert(\"RGB\") \n",
    "    elif input_type == 'pil':\n",
    "        im = frame\n",
    "    img_size = hash_size * high_freq_factor\n",
    "    image = im.convert(\"L\").resize((img_size, img_size), Image.ANTIALIAS)\n",
    "    pixels = np.asarray(image)\n",
    "    dct = scipy.fftpack.dct(scipy.fftpack.dct(pixels, axis=0), axis=1)\n",
    "    dctlowfreq = dct[:hash_size, :hash_size]\n",
    "    med = np.median(dctlowfreq)\n",
    "    diff = dctlowfreq > med\n",
    "    phash = [1 if x == True else 0 for x in diff.flatten()]\n",
    "    return phash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_test_image(frame_paths, n, method):\n",
    "    test_image = Image.open(frame_paths[n]).convert(\"RGB\") \n",
    "    im = np.asarray(test_image)\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    k, h, w, d = im.shape\n",
    "    im = method.augment_images(im)\n",
    "    im = im.reshape((im.shape[1], im.shape[2], im.shape[3]))\n",
    "    test_image_aug = Image.fromarray(im)\n",
    "    return test_image_aug\n",
    "\n",
    "def augment_and_hash(frame_path, method, hash_size, high_freq_factor):\n",
    "    im = Image.open(frame_path).convert(\"RGB\") \n",
    "    im = np.asarray(im)\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    k, h, w, d = im.shape\n",
    "    im = method.augment_images(im)\n",
    "    im = im.reshape((im.shape[1], im.shape[2], im.shape[3]))\n",
    "    im = Image.fromarray(im)\n",
    "    phash = get_hash(im, hash_size, high_freq_factor, input_type = 'pil')\n",
    "    return phash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    \n",
    "    def change_hash(dataset_column):\n",
    "    phashes = list(dataset_column)\n",
    "    new_phashes = []\n",
    "    for phash in phashes:\n",
    "        new_hash = np.array([int(e) for e in phash if e =='0' or e =='1'])\n",
    "        new_phashes.append(new_hash)\n",
    "    return new_phashes\n",
    "    \n",
    "    loc = globals()['_dh'][0] \n",
    "    path_file = loc + '/films/film_1/hashes/' + dataset_name\n",
    "    with open(path_file) as csvfile:\n",
    "        dataset = pd.read_csv(csvfile)\n",
    "    \n",
    "    columns = []\n",
    "    for column in dataset.columns:\n",
    "        if 'hash' in column:\n",
    "            dataset[column] = change_hash(dataset[column])\n",
    "    return dataset\n",
    "\n",
    "def write_dataset(dataset, dataset_name):\n",
    "    loc = globals()['_dh'][0] \n",
    "    path_file = loc + '/films/film_1/hashes/' + dataset_name\n",
    "    dataset.to_csv(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'SeeFoo-hashes-15-4.csv'\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_frame_paths = '/home/emsala/Documenten/Studie/These/phashing/dataset/films/film_1/frames/*'\n",
    "frame_paths = sorted(glob.glob(wd_frame_paths))\n",
    "\n",
    "hash_size = 15\n",
    "high_freq_factor = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "phashes_add = [augment_and_hash(frame_path, add, hash_size, high_freq_factor) for frame_path in frame_paths]\n",
    "phashes_gauss = [augment_and_hash(frame_path, gauss, hash_size, high_freq_factor) for frame_path in frame_paths]\n",
    "phashes_compress = [augment_and_hash(frame_path, compress, hash_size, high_freq_factor) for frame_path in frame_paths]\n",
    "phashes_add_hsv = [augment_and_hash(frame_path, add_hsv, hash_size, high_freq_factor) for frame_path in frame_paths]\n",
    "phashes_contrast = [augment_and_hash(frame_path, contrast, hash_size, high_freq_factor) for frame_path in frame_paths]\n",
    "phashes_resize = [augment_and_hash(frame_path, resize, hash_size, high_freq_factor) for frame_path in frame_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['hash_add'] = phashes_add\n",
    "dataset['hash_gauss'] = phashes_gauss\n",
    "dataset['hash_compress'] = phashes_compress\n",
    "dataset['hash_add_hsv'] = phashes_add_hsv\n",
    "dataset['hash_contrast'] = phashes_contrast\n",
    "dataset['hash_resize'] = phashes_resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataset(dataset, 'SeeFoo-hashes-15-4-augmented.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash\n",
      "hash_add\n",
      "hash_gauss\n",
      "hash_compress\n",
      "hash_add_hsv\n",
      "hash_contrast\n",
      "hash_resize\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('SeeFoo-hashes-15-4-augmented.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
